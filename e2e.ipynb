{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('src')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ssg2/miniconda3/envs/codeblip/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.cuda.amp import autocast as autocast\n",
    "from transformers import AutoTokenizer, logging\n",
    "\n",
    "from codeblip import CodeBlip\n",
    "from codeblip_qformer import CodeQformer\n",
    "# from modelling_t5 import T5Config, T5ForConditionalGeneration\n",
    "from transformers import AutoTokenizer, T5ForConditionalGeneration\n",
    "\n",
    "logging.set_verbosity_error()  # Only show errors, not warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "def update_loss_dict(loss_dict, new_loss_dict):\n",
    "    for key in new_loss_dict:\n",
    "        if key not in loss_dict:\n",
    "            loss_dict[key] = 0\n",
    "        loss_dict[key] += new_loss_dict[key].item()\n",
    "    return loss_dict\n",
    "\n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stage 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CodeQformer(CodeBlip):  # Inherits from Blip2Base\n",
    "    \"\"\"\n",
    "    CodeBlip Qformer model for code translation task.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_query_token=32,\n",
    "        cross_attention_freq=2,\n",
    "        embed_dim=256,\n",
    "        max_source_len=128,\n",
    "        max_target_len=128,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.code_encoder, self.ln_code = self.init_code_encoder()\n",
    "        # freeze the encoder\n",
    "        for param in self.code_encoder.parameters():\n",
    "            param.requires_grad = False\n",
    "        self.code_encoder.eval()\n",
    "        # self.code_encoder.train =\n",
    "\n",
    "\n",
    "        # Initialize the Qformer and Query Tokens\n",
    "        self.Qformer, self.query_tokens = self.init_Qformer(\n",
    "            num_query_token, embed_dim, cross_attention_freq\n",
    "        )\n",
    "        # Tokenizer for encoding source and target code\n",
    "        self.tokenizer = self.init_tokenizer()\n",
    "        self.Qformer.resize_token_embeddings(len(self.tokenizer))\n",
    "\n",
    "        # not sure what this does\n",
    "        state_dict = self.Qformer.state_dict()\n",
    "        for name, param in self.Qformer.named_parameters():\n",
    "            if \"_query\" in name:\n",
    "                key_orig = name.replace(\"_query\", \"\")\n",
    "                param.data.copy_(state_dict[key_orig])\n",
    "\n",
    "        # Projection layers for source and target code\n",
    "        self.source_proj = nn.Linear(self.Qformer.config.hidden_size, embed_dim)\n",
    "        self.target_proj = nn.Linear(self.Qformer.config.hidden_size, embed_dim)\n",
    "\n",
    "        # Temperature parameter for contrastive loss\n",
    "        self.temp = nn.Parameter(0.07 * torch.ones([]))\n",
    "\n",
    "        # Max lengths for source and target tokens\n",
    "        self.max_source_len = max_source_len\n",
    "        self.max_target_len = max_target_len\n",
    "\n",
    "        self.itm_head = nn.Linear(self.Qformer.config.hidden_size, 2)\n",
    "\n",
    "    def forward(self, samples):\n",
    "        source_code = samples[\"source_code\"] # image\n",
    "        target_code = samples[\"target_code\"] # text\n",
    "\n",
    "        # Process source code\n",
    "        source_tokens = self.tokenizer(\n",
    "            source_code, padding=\"max_length\", truncation=True,\n",
    "            max_length=self.max_source_len, return_tensors=\"pt\"\n",
    "        ).to(self.Qformer.device)\n",
    "\n",
    "        # code embedding\n",
    "        # source_output = self.ln_code(self.code_encoder(source_tokens.input_ids, attention_mask=source_tokens.attention_mask, return_dict=True).last_hidden_state)\n",
    "        source_output = self.ln_code(self.code_encoder(**source_tokens, return_dict=True).last_hidden_state)\n",
    "\n",
    "        # source_output = self.Qformer.bert(**source_tokens, return_dict=True)\n",
    "        # source_representations = F.normalize(\n",
    "        #     self.source_proj(source_output.last_hidden_state[:, 0, :]), dim=-1\n",
    "        # )\n",
    "\n",
    "        # expand query tokens to batch size\n",
    "        query_tokens = self.query_tokens.expand(source_output.shape[0], -1, -1)\n",
    "\n",
    "        query_output = self.Qformer.bert(\n",
    "            query_embeds=query_tokens,\n",
    "            encoder_hidden_states=source_output,\n",
    "            encoder_attention_mask=source_tokens.attention_mask,\n",
    "            use_cache=True,\n",
    "            return_dict=True,\n",
    "        )\n",
    "\n",
    "        source_features = F.normalize(\n",
    "            self.source_proj(query_output.last_hidden_state), dim=-1\n",
    "        )\n",
    "\n",
    "        # Process target code\n",
    "        target_tokens = self.tokenizer(\n",
    "            target_code, padding=\"max_length\", truncation=True,\n",
    "            max_length=self.max_target_len, return_tensors=\"pt\"\n",
    "        ).to(self.Qformer.device)\n",
    "        target_output = self.Qformer.bert(target_tokens.input_ids, attention_mask=target_tokens.attention_mask, return_dict=True)\n",
    "\n",
    "        target_features = F.normalize(\n",
    "            self.target_proj(target_output.last_hidden_state[:, 0, :]), dim=-1\n",
    "        )\n",
    "\n",
    "        # Contrastive learning\n",
    "        # sim_matrix = torch.matmul(source_features, target_features.T) / self.temp\n",
    "        # loss_contrastive = F.cross_entropy(sim_matrix, torch.arange(source_features.size(0), device=sim_matrix.device))\n",
    "        #\n",
    "        # return loss_contrastive\n",
    "\n",
    "        # Contrastive learning\n",
    "        # image feat = source_features\n",
    "        # text feat = target_features\n",
    "\n",
    "        source_features_all = source_features\n",
    "        target_features_all = target_features\n",
    "\n",
    "        #\n",
    "        sim_q2t = torch.matmul(source_features.unsqueeze(1), target_features_all.unsqueeze(-1)).squeeze()\n",
    "\n",
    "        # source-target similarity\n",
    "        sim_s2t, _ = sim_q2t.max(-1)\n",
    "        sim_s2t /= self.temp\n",
    "\n",
    "        # sim_t2q = torch.matmul(target_features.unsqueeze(1), source_features_all.unsqueeze(-1)).squeeze()\n",
    "        sim_t2q = torch.matmul(\n",
    "            target_features.unsqueeze(1).unsqueeze(1), source_features_all.permute(0, 2, 1)\n",
    "        ).squeeze()\n",
    "        # target-source similarity ; aggregate the max across all query tokens\n",
    "        sim_t2s, _ = sim_t2q.max(-1)\n",
    "        sim_t2s /= self.temp\n",
    "\n",
    "        # rank = dist.get_rank()\n",
    "        rank = 0 # something to do with distributed training, but we're not using it so just set to 0\n",
    "        bs = source_features.size(0)\n",
    "        targets = torch.linspace(rank * bs, rank * bs + bs - 1, bs, dtype=int).to(sim_s2t.device)\n",
    "\n",
    "        loss_stc = (F.cross_entropy(sim_s2t, targets, label_smoothing=0.1) + F.cross_entropy(sim_t2s, targets, label_smoothing=0.1)) / 2\n",
    "\n",
    "        # return loss_stc\n",
    "\n",
    "        # Sorce - Target Matching\n",
    "        # image feat = source_features\n",
    "        # text feat = target_features\n",
    "        target_input_ids_world = target_tokens.input_ids # if distributed, this is all_gather(target_tokens.input_ids)\n",
    "        target_attention_mask_world = target_tokens.attention_mask # if distributed, this is all_gather(target_tokens.attention_mask)\n",
    "        # image_embeds_world = all_gather_with_grad(image_embeds)\n",
    "        source_features_world = source_features_all # if distributed, this is all_gather_with_grad(source_features_all)\n",
    "        with torch.no_grad():\n",
    "            # if \"image_id\" in samples.keys():\n",
    "            #     mask = torch.eq(image_ids, image_ids_all.t())\n",
    "            #     sim_t2i.masked_fill_(mask, -10000)\n",
    "            #     sim_i2t.masked_fill_(mask, -10000)\n",
    "            # else:\n",
    "            sim_s2t[:, rank * bs: rank * bs + bs].fill_diagonal_(-10000)\n",
    "            sim_t2s[:, rank * bs: rank * bs + bs].fill_diagonal_(-10000)\n",
    "\n",
    "            weights_t2s = F.softmax(sim_t2s, dim=1)\n",
    "            weights_s2t = F.softmax(sim_s2t, dim=1)\n",
    "\n",
    "        # select a negative image for each text\n",
    "        source_embeds_neg = []\n",
    "        for b in range(bs):\n",
    "            neg_idx = torch.multinomial(weights_t2s[b], 1).item()\n",
    "            source_embeds_neg.append(source_features_world[neg_idx])\n",
    "        source_embeds_neg = torch.stack(source_embeds_neg, dim=0)\n",
    "\n",
    "        # select a negative text for each image\n",
    "        target_ids_neg = []\n",
    "        target_atts_neg = []\n",
    "        for b in range(bs):\n",
    "            neg_idx = torch.multinomial(weights_s2t[b], 1).item()\n",
    "            target_ids_neg.append(target_input_ids_world[neg_idx])\n",
    "            target_atts_neg.append(target_attention_mask_world[neg_idx])\n",
    "\n",
    "        target_ids_neg = torch.stack(target_ids_neg, dim=0)\n",
    "        target_atts_neg = torch.stack(target_atts_neg, dim=0)\n",
    "\n",
    "        target_ids_all = torch.cat(\n",
    "            [target_tokens.input_ids, target_tokens.input_ids, target_ids_neg], dim=0\n",
    "        )  # pos, pos, neg\n",
    "        target_atts_all = torch.cat(\n",
    "            [target_tokens.attention_mask, target_tokens.attention_mask, target_atts_neg],\n",
    "            dim=0,\n",
    "        )\n",
    "\n",
    "        query_tokens_itm = self.query_tokens.expand(target_ids_all.shape[0], -1, -1)\n",
    "        query_atts_itm = torch.ones(query_tokens_itm.size()[:-1], dtype=torch.long).to(\n",
    "            self.Qformer.device\n",
    "        )\n",
    "        attention_mask_all = torch.cat([query_atts_itm, target_atts_all], dim=1)\n",
    "\n",
    "        source_embeds_all = torch.cat(\n",
    "            [source_features, source_embeds_neg, source_features], dim=0\n",
    "        )  # pos, neg, pos\n",
    "        source_atts_all = torch.ones(source_embeds_all.size()[:-1], dtype=torch.long).to(\n",
    "            self.Qformer.device\n",
    "        )\n",
    "\n",
    "        output_itm = self.Qformer.bert(\n",
    "            target_ids_all,\n",
    "            query_embeds=query_tokens_itm,\n",
    "            attention_mask=attention_mask_all,\n",
    "            encoder_hidden_states=source_embeds_all,\n",
    "            encoder_attention_mask=source_atts_all,\n",
    "            return_dict=True,\n",
    "        )\n",
    "\n",
    "        vl_embeddings = output_itm.last_hidden_state[:, : query_tokens_itm.size(1), :]\n",
    "        vl_output = self.itm_head(vl_embeddings)\n",
    "        logits = vl_output.mean(dim=1)\n",
    "\n",
    "        itm_labels = torch.cat(\n",
    "            [torch.ones(bs, dtype=torch.long), torch.zeros(2 * bs, dtype=torch.long)],\n",
    "            dim=0,\n",
    "        ).to(self.Qformer.device)\n",
    "        loss_stm = F.cross_entropy(logits, itm_labels)\n",
    "\n",
    "\n",
    "        # 3rd loss\n",
    "        decoder_input_ids = target_tokens.input_ids.clone()\n",
    "        decoder_input_ids[:, 0] = self.tokenizer.bos_token_id\n",
    "        labels = decoder_input_ids.masked_fill(\n",
    "            decoder_input_ids == self.tokenizer.pad_token_id, -100\n",
    "        )\n",
    "\n",
    "        query_atts = torch.ones(query_tokens.size()[:-1], dtype=torch.long).to(\n",
    "            self.Qformer.device\n",
    "        )\n",
    "        attention_mask = torch.cat([query_atts, target_tokens.attention_mask], dim=1)\n",
    "        lm_output = self.Qformer(\n",
    "            decoder_input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            past_key_values=query_output.past_key_values,\n",
    "            return_dict=True,\n",
    "            labels=labels,\n",
    "        )\n",
    "\n",
    "        loss_lm = lm_output.loss\n",
    "\n",
    "        total_loss =  loss_stc + loss_stm + loss_lm\n",
    "\n",
    "        return {\n",
    "            \"loss\": total_loss,\n",
    "            \"loss_stc\": loss_stc,\n",
    "            \"loss_stm\": loss_stm,\n",
    "            \"loss_lm\": loss_lm,\n",
    "        }\n",
    "\n",
    "\n",
    "\n",
    "    @classmethod\n",
    "    def from_config(cls, cfg):\n",
    "        num_query_token = cfg.get(\"num_query_token\", 32)\n",
    "        cross_attention_freq = cfg.get(\"cross_attention_freq\", 2)\n",
    "        embed_dim = cfg.get(\"embed_dim\", 768)\n",
    "        max_source_len = cfg.get(\"max_source_len\", 512)\n",
    "        max_target_len = cfg.get(\"max_target_len\", 512)\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        model = cls(\n",
    "            num_query_token=num_query_token,\n",
    "            cross_attention_freq=cross_attention_freq,\n",
    "            embed_dim=embed_dim,\n",
    "            max_source_len=max_source_len,\n",
    "            max_target_len=max_target_len,\n",
    "        ).to(device)\n",
    "        pretrained_path = cfg.get(\"pretrained_path\", os.path.join(\"models\", \"stage1_out\", \"stage1_best.pt\"))\n",
    "        model.load_state_dict(torch.load(pretrained_path))\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[[ 0.0137,  0.0076, -0.0196,  ..., -0.0229, -0.0487,  0.0145],\n",
      "         [-0.0258, -0.0233, -0.0146,  ...,  0.0117, -0.0187, -0.0115],\n",
      "         [ 0.0406,  0.0035, -0.0143,  ..., -0.0202,  0.0014, -0.0066],\n",
      "         ...,\n",
      "         [-0.0292, -0.0065, -0.0202,  ...,  0.0041, -0.0126, -0.0023],\n",
      "         [-0.0280, -0.0186, -0.0026,  ...,  0.0067,  0.0158,  0.0110],\n",
      "         [-0.0106, -0.0122,  0.0075,  ...,  0.0217,  0.0329,  0.0013]]],\n",
      "       requires_grad=True) torch.Size([1, 32, 768])\n",
      "Loss from forward pass: 12.496234893798828\n"
     ]
    }
   ],
   "source": [
    "def stage1_test():\n",
    "    model = CodeQformer(num_query_token=32, cross_attention_freq=2, embed_dim=768, max_source_len=512, max_target_len=512)\n",
    "\n",
    "    stage1_pretrained_path = os.path.join(\"models\", \"stage1_out\", \"stage1_best.pt\")\n",
    "    # model = CodeQformer.from_config({\n",
    "    #     \"num_query_token\": 32,\n",
    "    #     \"cross_attention_freq\": 2,\n",
    "    #     \"embed_dim\": 768,\n",
    "    #     \"max_source_len\": 512,\n",
    "    #     \"max_target_len\": 512,\n",
    "    #     \"pretrained_path\": stage1_pretrained_path\n",
    "    # })\n",
    "\n",
    "    print(model.query_tokens, model.query_tokens.shape)\n",
    "\n",
    "    # Dummy source and target code samples\n",
    "    source_code_samples = [\n",
    "        \"public class HelloWorld { public static void main(String[] args) { System.out.println(\\\"Hello, world!\\\"); } }\",\n",
    "        \"public class Test { public static int add(int a, int b) { return a + b; } }\"]\n",
    "    target_code_samples = [\n",
    "        \"class HelloWorld { static void Main(string[] args) { Console.WriteLine(\\\"Hello, world!\\\"); } }\",\n",
    "        \"class Test { static int Add(int a, int b) { return a + b; } }\"]\n",
    "\n",
    "    # Prepare input for the model\n",
    "    samples = {\"source_code\": source_code_samples, \"target_code\": target_code_samples}\n",
    "\n",
    "    # Perform a forward pass\n",
    "    losses = model(samples)\n",
    "    print(f\"Loss from forward pass: {losses['loss'].item()}\")\n",
    "\n",
    "stage1_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Trained\n",
    "\n",
    "# Parameter containing:\n",
    "# tensor([[[ 0.0353, -0.0337, -0.0014,  ...,  0.0036,  0.0390, -0.0232],\n",
    "#          [-0.0112,  0.0043,  0.0010,  ..., -0.0294, -0.0440, -0.0227],\n",
    "#          [-0.0105, -0.0109, -0.0025,  ...,  0.0016, -0.0248, -0.0081],\n",
    "#          ...,\n",
    "#          [ 0.0352,  0.0090,  0.0372,  ...,  0.0266, -0.0092, -0.0088],\n",
    "#          [ 0.0177,  0.0234,  0.0203,  ..., -0.0191, -0.0187,  0.0254],\n",
    "#          [-0.0103,  0.0153,  0.0066,  ...,  0.0129, -0.0445, -0.0137]]],\n",
    "#        device='cuda:0', requires_grad=True) torch.Size([1, 32, 768])\n",
    "# Loss from forward pass: 6.058288097381592"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stage 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['public static void Main(string[] args){throw new System.NotImplementedException();}']\n",
      "['public static void Main(string[] args){WriteLine(\"Hello, world!\");}']\n"
     ]
    }
   ],
   "source": [
    "def codet5test():\n",
    "    from transformers import AutoModelForSeq2SeqLM, AutoTokenizer, T5ForConditionalGeneration, RobertaTokenizer\n",
    "    import torch\n",
    "\n",
    "    checkpoint = \"Salesforce/codet5-base\"\n",
    "    sp_checkpoint = \"Salesforce/codet5-base-codexglue-translate-java-cs\"\n",
    "    device = \"cuda\" # for GPU usage or \"cpu\" for CPU usage\n",
    "\n",
    "    # task_prefix = 'Translate Java to Python:'\n",
    "    task_prefix = 'Translate Java to C#:'\n",
    "\n",
    "    tokenizer = RobertaTokenizer.from_pretrained(checkpoint)\n",
    "    model = T5ForConditionalGeneration.from_pretrained(sp_checkpoint,\n",
    "                                                torch_dtype=torch.float32,\n",
    "                                                low_cpu_mem_usage=True,\n",
    "                                                trust_remote_code=True,\n",
    "                                                early_stopping=False,\n",
    "                                                ).to(device)\n",
    "\n",
    "    # text = \":\"\n",
    "    # sample_java_code = \"public void serialize(LittleEndianOutput out) {out.writeShort(field_1_vcenter);}\"\n",
    "    # hello world\n",
    "    sample_java_code_with_class = \"public class HelloWorld{ public static void main(String[] args) {System.out.println(\\\"Hello, world!\\\"); }\"\n",
    "    sample_java_code_without_class = \"public static void main(String[] args) {System.out.println(\\\"Hello, world!\\\"); }\"\n",
    "    # sample_java_code = ': if x==0: x += 1\"'\n",
    "\n",
    "    encoding = tokenizer([task_prefix + sample_java_code_with_class], return_tensors=\"pt\", padding=True).to(device)\n",
    "    # encoding['decoder_input_ids'] = encoding['input_ids'].clone()\n",
    "    outputs = model.generate(input_ids=encoding['input_ids'], attention_mask=encoding['attention_mask'], do_sample=False, max_length=750)\n",
    "    print(tokenizer.batch_decode(outputs, skip_special_tokens=True))\n",
    "\n",
    "    encoding = tokenizer([task_prefix + sample_java_code_without_class], return_tensors=\"pt\", padding=True).to(device)\n",
    "    # encoding['decoder_input_ids'] = encoding['input_ids'].clone()\n",
    "    outputs = model.generate(input_ids=encoding['input_ids'], attention_mask=encoding['attention_mask'], do_sample=False, max_length=750)\n",
    "    print(tokenizer.batch_decode(outputs, skip_special_tokens=True))\n",
    "\n",
    "codet5test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CodeBlipT5(CodeBlip):\n",
    "    def __init__(self, qformer, query_tokens, t5_tokenizer, t5_model, prompt, max_source_len=512,\n",
    "        max_target_len=512, num_query_token=32, embed_dim=768, cross_attention_freq=2):\n",
    "        super().__init__()\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        print(f\"Device: {self.device}\")\n",
    "\n",
    "        self.tokenizer = self.init_tokenizer()\n",
    "        self.code_encoder, self.ln_code = self.init_code_encoder()\n",
    "\n",
    "        # self.code_encoder.to(self.device)\n",
    "        # self.ln_code.to(self.device)\n",
    "\n",
    "        # freeze the encoder\n",
    "        for param in self.code_encoder.parameters():\n",
    "            param.requires_grad = False\n",
    "        self.code_encoder.eval()\n",
    "\n",
    "        self.max_source_len = max_source_len\n",
    "        self.max_target_len = max_target_len\n",
    "\n",
    "        if qformer is not None and query_tokens is not None:\n",
    "            self.Qformer = qformer\n",
    "            self.query_tokens = query_tokens\n",
    "        else:\n",
    "            self.Qformer, self.query_tokens = self.init_Qformer(num_query_token, embed_dim, cross_attention_freq)\n",
    "\n",
    "        # not sure if this is needed\n",
    "        # self.Qformer.cls = None\n",
    "        # self.Qformer.bert.embeddings.word_embeddings = None\n",
    "        # self.Qformer.bert.embeddings.position_embeddings = None\n",
    "        # for layer in self.Qformer.bert.encoder.layer:\n",
    "        #     layer.output = None\n",
    "        #     layer.intermediate = None\n",
    "\n",
    "        self.Qformer.to(self.device)\n",
    "\n",
    "\n",
    "        # self.t5_tokenizer = T5TokenizerFast.from_pretrained(t5_model)\n",
    "        # self.t5_tokenizer = AutoTokenizer.from_pretrained(t5_model)\n",
    "        self.t5_tokenizer = t5_tokenizer\n",
    "        # t5_config = T5Config.from_pretrained(t5_model)\n",
    "        # t5_config.dense_act_fn = \"gelu\"\n",
    "        # self.t5_model = T5ForConditionalGeneration.from_pretrained(\n",
    "        #     t5_model, config=t5_config\n",
    "        # )\n",
    "\n",
    "        # self.t5_model = T5ForConditionalGeneration.from_pretrained(t5_model)\n",
    "        self.t5_model = t5_model\n",
    "\n",
    "        for param in self.t5_model.parameters():\n",
    "            param.requires_grad = False\n",
    "            # param.data = param.data.float16()\n",
    "            # make it fp16\n",
    "            param.data = param.data.half()\n",
    "\n",
    "        self.t5_proj = nn.Linear(\n",
    "            self.Qformer.config.hidden_size, self.t5_model.config.hidden_size\n",
    "        )\n",
    "\n",
    "        # self.max_txt_len = max_txt_len\n",
    "        self.prompt = prompt\n",
    "\n",
    "\n",
    "    def forward(self, samples):\n",
    "        source_code = samples[\"source_code\"] # image\n",
    "        # target_code = samples[\"target_code\"] # text\n",
    "\n",
    "        # Process source code\n",
    "        source_tokens = self.tokenizer(\n",
    "            source_code, padding=\"max_length\", truncation=True,\n",
    "            max_length=self.max_source_len, return_tensors=\"pt\"\n",
    "        ).to(self.device)\n",
    "\n",
    "        # code embedding\n",
    "        source_output = self.ln_code(self.code_encoder(**source_tokens, return_dict=True).last_hidden_state)\n",
    "\n",
    "        # expand query tokens to batch size\n",
    "        query_tokens = self.query_tokens.expand(source_output.shape[0], -1, -1)\n",
    "\n",
    "        query_output = self.Qformer.bert(\n",
    "            query_embeds=query_tokens,\n",
    "            encoder_hidden_states=source_output,\n",
    "            encoder_attention_mask=source_tokens.attention_mask,\n",
    "            use_cache=True,\n",
    "            return_dict=True,\n",
    "        )\n",
    "\n",
    "        inputs_t5 = self.t5_proj(query_output.last_hidden_state)\n",
    "        atts_t5 = torch.ones(inputs_t5.size()[:-1], dtype=torch.long).to(self.device)\n",
    "\n",
    "\n",
    "        with self.maybe_autocast(dtype=torch.float16):\n",
    "            input_tokens = self.t5_tokenizer(\n",
    "                samples['target_code_input'],\n",
    "                padding=\"longest\",\n",
    "                truncation=True,\n",
    "                max_length=self.max_source_len,\n",
    "                return_tensors=\"pt\",\n",
    "            ).to(self.device)\n",
    "            output_tokens = self.t5_tokenizer(\n",
    "                samples['target_code_output'],\n",
    "                padding=\"longest\",\n",
    "                truncation=True,\n",
    "                max_length=self.max_target_len,\n",
    "                return_tensors=\"pt\",\n",
    "            ).to(self.device)\n",
    "\n",
    "        encoder_atts = torch.cat([atts_t5, input_tokens.attention_mask], dim=1)\n",
    "\n",
    "        targets = output_tokens.input_ids.masked_fill(\n",
    "            output_tokens.input_ids == self.t5_tokenizer.pad_token_id, -100\n",
    "        )\n",
    "\n",
    "        inputs_embeds = self.t5_model.encoder.embed_tokens(input_tokens.input_ids)\n",
    "        inputs_embeds = torch.cat([inputs_t5, inputs_embeds], dim=1)\n",
    "\n",
    "        outputs = self.t5_model(\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            attention_mask=encoder_atts,\n",
    "            decoder_attention_mask=output_tokens.attention_mask,\n",
    "            return_dict=True,\n",
    "            labels=targets,\n",
    "        )\n",
    "        loss = outputs.loss\n",
    "\n",
    "        return {\"loss\": loss}\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def generate(\n",
    "        self,\n",
    "        samples,\n",
    "        use_nucleus_sampling=False,\n",
    "        num_beams=5,\n",
    "        max_length=30,\n",
    "        min_length=1,\n",
    "        top_p=0.9,\n",
    "        repetition_penalty=1.0,\n",
    "        length_penalty=0.9,\n",
    "        num_captions=1,\n",
    "        temperature=1,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            samples (dict): A dictionary containing the following keys:\n",
    "                - image (torch.Tensor): A tensor of shape (batch_size, 3, H, W)\n",
    "            use_nucleus_sampling (bool): Whether to use nucleus sampling. If False, use top-k sampling.\n",
    "            num_beams (int): Number of beams for beam search. 1 means no beam search.\n",
    "            max_length (int): The maximum length of the sequence to be generated.\n",
    "            min_length (int): The minimum length of the sequence to be generated.\n",
    "            top_p (float): The cumulative probability for nucleus sampling.\n",
    "            repetition_penalty (float): The parameter for repetition penalty. 1.0 means no penalty.\n",
    "            num_captions (int): Number of captions to be generated for each image.\n",
    "        Returns:\n",
    "            captions (list): A list of strings of length batch_size * num_captions.\n",
    "        \"\"\"\n",
    "\n",
    "        source_code = samples[\"source_code\"] # image\n",
    "\n",
    "        source_tokens = self.tokenizer(\n",
    "            source_code, padding=\"max_length\", truncation=True,\n",
    "            max_length=self.max_source_len, return_tensors=\"pt\"\n",
    "        ).to(self.device)\n",
    "\n",
    "        source_output = self.ln_code(self.code_encoder(**source_tokens, return_dict=True).last_hidden_state)\n",
    "        query_tokens = self.query_tokens.expand(source_output.shape[0], -1, -1)\n",
    "\n",
    "        query_output = self.Qformer.bert(\n",
    "            query_embeds=query_tokens,\n",
    "            encoder_hidden_states=source_output,\n",
    "            encoder_attention_mask=source_tokens.attention_mask,\n",
    "            return_dict=True,\n",
    "        )\n",
    "\n",
    "        inputs_t5 = self.t5_proj(query_output.last_hidden_state)\n",
    "        atts_t5 = torch.ones(inputs_t5.size()[:-1], dtype=torch.long).to(self.device)\n",
    "\n",
    "\n",
    "        # if \"prompt\" in samples.keys():\n",
    "        #     prompt = samples[\"prompt\"]\n",
    "        # else:\n",
    "        #     prompt = self.prompt\n",
    "\n",
    "        # if isinstance(prompt, str):\n",
    "        #     prompt = [prompt] * image.size(0)\n",
    "        # else:\n",
    "        #     assert len(prompt) == image.size(\n",
    "        #         0\n",
    "        #     ), \"The number of prompts must be equal to the batch size.\"\n",
    "\n",
    "        # input_tokens = self.t5_tokenizer(\n",
    "        #     prompt, padding=\"longest\", return_tensors=\"pt\"\n",
    "        # ).to(image.device)\n",
    "        input_tokens = self.t5_tokenizer(\n",
    "            self.prompt,\n",
    "            padding=\"longest\",\n",
    "            return_tensors=\"pt\",\n",
    "        ).to(self.device)\n",
    "\n",
    "        encoder_atts = torch.cat([atts_t5, input_tokens.attention_mask], dim=1)\n",
    "\n",
    "        with self.maybe_autocast(dtype=torch.float16):\n",
    "            inputs_embeds = self.t5_model.encoder.embed_tokens(input_tokens.input_ids)\n",
    "            inputs_embeds = torch.cat([inputs_t5, inputs_embeds], dim=1)\n",
    "\n",
    "            outputs = self.t5_model.generate(\n",
    "                inputs_embeds=inputs_embeds,\n",
    "                attention_mask=encoder_atts,\n",
    "                do_sample=use_nucleus_sampling,\n",
    "                top_p=top_p,\n",
    "                temperature=temperature,\n",
    "                num_beams=num_beams,\n",
    "                max_new_tokens=max_length,\n",
    "                min_length=min_length,\n",
    "                repetition_penalty=repetition_penalty,\n",
    "                length_penalty=length_penalty,\n",
    "                num_return_sequences=num_captions,\n",
    "            )\n",
    "            output_text = self.t5_tokenizer.batch_decode(\n",
    "                outputs, skip_special_tokens=True\n",
    "            )\n",
    "\n",
    "        return output_text[0]\n",
    "\n",
    "    @classmethod\n",
    "    def from_config(cls, config):\n",
    "        checkpoint = config['pretrained_path']\n",
    "        stage1_model = CodeQformer.from_config({'pretrained_path': checkpoint})\n",
    "\n",
    "        stage1_qformer = stage1_model.Qformer\n",
    "        stage1_query_tokens = stage1_model.query_tokens\n",
    "\n",
    "        return cls(stage1_qformer, stage1_query_tokens, **config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n",
      "{'source_code': ['public class HelloWorld { public static void main(String[] args) { System.out.println(\"Hello, world!\"); } }', 'public class Test { public static int add(int a, int b) { return a + b; } }'], 'target_code_input': ['class HelloWorld { static void Main(', 'class Test { sta'], 'target_code_output': ['string[] args) { Console.WriteLine(\"Hello, world!\"); } }', 'tic int Add(int a, int b) { return a + b; } }']}\n",
      "Loss from forward pass: 5.6953125\n"
     ]
    }
   ],
   "source": [
    "def stage2_test():\n",
    "    stage_1_checkpoint = 'models/stage1_out/stage1_best.pt'\n",
    "    stage1_model = CodeQformer.from_config({'pretrained_path': stage_1_checkpoint})\n",
    "\n",
    "    stage1_qformer = stage1_model.Qformer\n",
    "    stage1_query_tokens = stage1_model.query_tokens\n",
    "\n",
    "    source_lang='java'\n",
    "    target_lang='cs'\n",
    "\n",
    "    t5_tokenizer = AutoTokenizer.from_pretrained('Salesforce/codet5-base')\n",
    "    t5_model = T5ForConditionalGeneration.from_pretrained(f'Salesforce/codet5-base-codexglue-translate-{source_lang}-{target_lang}')\n",
    "\n",
    "    # prompt = f'Translate {source_lang} to {target_lang}'\n",
    "    prompt = ''\n",
    "\n",
    "\n",
    "\n",
    "    model = CodeBlipT5(stage1_qformer, stage1_query_tokens, t5_tokenizer=t5_tokenizer, t5_model=t5_model, prompt=prompt).to('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    # stage_2_checkpoint = os.path.join(\"models\", \"stage2_out\", \"stage2_best.pt\")\n",
    "    # model.load_state_dict(torch.load(stage_2_checkpoint))\n",
    "    # Dummy source and target code samples\n",
    "    source_code_samples = [\n",
    "        \"public class HelloWorld { public static void main(String[] args) { System.out.println(\\\"Hello, world!\\\"); } }\",\n",
    "        \"public class Test { public static int add(int a, int b) { return a + b; } }\"]\n",
    "    target_code_samples = [\n",
    "        \"class HelloWorld { static void Main(string[] args) { Console.WriteLine(\\\"Hello, world!\\\"); } }\",\n",
    "        \"class Test { static int Add(int a, int b) { return a + b; } }\"]\n",
    "\n",
    "    # split each sample of target code into two parts: target_code_input and target_code_output\n",
    "    tci = []\n",
    "    tco = []\n",
    "    for i in range(len(target_code_samples)):\n",
    "        middle = random.randint(1, len(target_code_samples[i]))\n",
    "        target_code_input = target_code_samples[i][:middle]\n",
    "        target_code_output = target_code_samples[i][middle:]\n",
    "\n",
    "        tci.append(target_code_input)\n",
    "        tco.append(target_code_output)\n",
    "\n",
    "    # Prepare input for the model\n",
    "    # samples = {\"source_code\": source_code_samples, \"target_code\": target_code_samples}\n",
    "    samples = {\"source_code\": source_code_samples, \"target_code_input\": tci, \"target_code_output\": tco}\n",
    "    print(samples)\n",
    "    # Perform a forward pass\n",
    "    losses = model(samples)\n",
    "    print(f\"Loss from forward pass: {losses['loss'].item()}\")\n",
    "\n",
    "stage2_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Device: cuda\n",
    "Loss from forward pass: 30.375"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n",
      "Current Sample: public static void main(String[] args) { System.out.println(\"Hello, world!\"); }\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ssg2/miniconda3/envs/codeblip/lib/python3.8/site-packages/transformers/generation/configuration_utils.py:386: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Output: public override void SetQuality(int quality){if (quality >= 0.0f){return;}if (quality >= 0.0f){return;}if (quality >= 0.0f){return;}if (quality >= 0.0f){return;}if (quality >= 0.0f){return;}\n",
      "\n",
      "Current Sample: public class Test { public static int add(int a, int b) { return a + b; } }\n",
      "Current Output: public override void Finish(object o){if (o == null){throw new ArgumentNullException(\"o\");}else{o = new object(o);}}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def inference_test():\n",
    "    stage_1_checkpoint = 'models/stage1_out/stage1_best.pt'\n",
    "    stage1_model = CodeQformer.from_config({'pretrained_path': stage_1_checkpoint})\n",
    "\n",
    "    stage1_qformer = stage1_model.Qformer\n",
    "    stage1_query_tokens = stage1_model.query_tokens\n",
    "\n",
    "    source_lang='java'\n",
    "    target_lang='cs'\n",
    "\n",
    "    t5_tokenizer = AutoTokenizer.from_pretrained('Salesforce/codet5-base')\n",
    "    t5_model = T5ForConditionalGeneration.from_pretrained(f'Salesforce/codet5-base-codexglue-translate-{source_lang}-{target_lang}')\n",
    "\n",
    "    # prompt = f'Translate {source_lang} to {target_lang}'\n",
    "    prompt = ''\n",
    "\n",
    "\n",
    "    model = CodeBlipT5(stage1_qformer, stage1_query_tokens, t5_tokenizer=t5_tokenizer, t5_model=t5_model, prompt=prompt).to('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    # stage_2_checkpoint = os.path.join(\"models\", \"stage2_out\", \"stage2_best.pt\")\n",
    "    # model.load_state_dict(torch.load(stage_2_checkpoint))\n",
    "    # Dummy source and target code samples\n",
    "    source_code_samples = [\n",
    "        \"public static void main(String[] args) { System.out.println(\\\"Hello, world!\\\"); }\",\n",
    "        \"public class Test { public static int add(int a, int b) { return a + b; } }\"]\n",
    "\n",
    "    for source_code in source_code_samples:\n",
    "        # Prepare input for the model\n",
    "        # samples = {\"source_code\": source_code_samples, \"target_code\": target_code_samples}\n",
    "        samples = {\"source_code\": [source_code]}\n",
    "        print(f'Current Sample: {source_code}')\n",
    "        # Perform a forward pass\n",
    "        output = model.generate(samples, max_length=750)\n",
    "        print(f\"Current Output: {output}\")\n",
    "        print()\n",
    "\n",
    "inference_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_code_samples = [\n",
    "    \"public class HelloWorld { public static void main(String[] args) { System.out.println(\\\"Hello, world!\\\"); } }\",\n",
    "    \"public class Test { public static int add(int a, int b) { return a + b; } }\",\n",
    "    \"public class Main { public static void main(String[] args) { int a = 5; int b = 10; System.out.println(a + b); } }\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "public class HelloWorld { public static void main(String[] args) { System.out.println(\"Hello, world!\"); } }\n",
      "[';;}.}']\n",
      "public class Test { public static int add(int a, int b) { return a + b; } }\n",
      "[';;']\n",
      "public class Main { public static void main(String[] args) { int a = 5; int b = 10; System.out.println(a + b); } }\n",
      "['']\n"
     ]
    }
   ],
   "source": [
    "for sample in source_code_samples:\n",
    "    print(sample)\n",
    "    print(model.generate({'source_code': sample}, use_nucleus_sampling=True, num_beams=5, max_length=30, min_length=1, top_p=0.9, repetition_penalty=1.0, length_penalty=1.0, num_captions=1, temperature=1))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
